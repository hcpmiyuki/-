{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 詳解ディープラーニング3章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    '''\n",
    "    ロジスティック回帰\n",
    "    '''\n",
    "    \n",
    "    #　パラメータ初期化　このパラメータを更新していく\n",
    "    def __init__(self, input_dim):\n",
    "#         self.input_dim = input_dim\n",
    "        self.w = np.random.normal(size=(input_dim,))\n",
    "        self.b = 0.\n",
    "       \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return sigmoid(np.matmul(x, self.w) + self.b)\n",
    "    \n",
    "    def compute_gradients(self, x, t):\n",
    "        # 誤差\n",
    "        delta = self.forward(x) - t\n",
    "        # 勾配\n",
    "        dw = np.matmul(x.T , delta)\n",
    "        db = np.matmul(np.ones(x.shape[0]), delta)\n",
    "        return dw, db\n",
    "    \n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss:  3.122\n",
      "epoch: 11, loss:  1.823\n",
      "epoch: 21, loss:  1.534\n",
      "epoch: 31, loss:  1.354\n",
      "epoch: 41, loss:  1.213\n",
      "epoch: 51, loss:  1.097\n",
      "epoch: 61, loss:  1.000\n",
      "epoch: 71, loss:  0.918\n",
      "epoch: 81, loss:  0.848\n",
      "epoch: 91, loss:  0.788\n",
      "epoch: 100, loss:  0.740\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    '''\n",
    "    1. データの準備\n",
    "    '''\n",
    "    # or\n",
    "    x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    t = np.array([0, 1, 1, 1])\n",
    "    \n",
    "    '''\n",
    "    2.モデルの構築\n",
    "    '''\n",
    "    model = LogisticRegression(input_dim=2)\n",
    "    '''\n",
    "    3.モデルの学習\n",
    "    '''\n",
    "    \n",
    "    def compute_loss(t, y):\n",
    "        return (-t * np.log(y) - (1 - t) * np.log(1 - y)).sum()\n",
    "    \n",
    "    def train_step(t, x):\n",
    "        y = model(x)\n",
    "        dw, db = model.compute_gradients(x, t)\n",
    "        # パラメータ更新\n",
    "        model.w = model.w - 0.1* dw\n",
    "        model.b = model.b - 0.1 * db\n",
    "#         model.w = model.w - dw\n",
    "#         model.b = model.b - db\n",
    "        # 損失求める←目的関数\n",
    "        loss = compute_loss(t, y)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    epochs = 100\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_step(t, x) # バッチ学習\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print('epoch: {}, loss: {: .3f}'.format(\n",
    "                epoch+1, \n",
    "                train_loss\n",
    "            ))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "0.3608810957831994\n",
      "[0 1]\n",
      "0.8996948426020324\n",
      "[1 0]\n",
      "0.8438700392090897\n",
      "[1 1]\n",
      "0.9884869045582483\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "4. モデルの評価\n",
    "'''\n",
    "for input in x:\n",
    "    print(input)\n",
    "    print(model.forward(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多クラスロジスティック回帰\n",
    "+ 実装は二値分類とさほど変わらない. 重みが行列になるが、outputの次元に合わせてinputを拡張する、みたいな気持ちでいいのか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多層パーセプトロン\n",
    "+ 非線形の分類を隠れ層を増やすことで可能にする\n",
    "#### 設計\n",
    "+ 入力-隠れ, 隠れ-出力の二つの層に分かれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, input_dim, output_dim, activation, deactivation):\n",
    "        self.W = np.random.normal(size=(input_dim, output_dim))\n",
    "        self.b = np.zeros(output_dim)\n",
    "        self.activation = activation\n",
    "        self.deactivation = deactivation\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self._input = x\n",
    "        self._pre_activation = np.matmul(x, self.W) + self.b\n",
    "        return self.activation(self._pre_activation)\n",
    "    \n",
    "    def backward(self, delta, W):\n",
    "        # 行きと逆の処理をする!\n",
    "        delta = self.deactivation(self._pre_activation)*np.matmul(delta, W.T) \n",
    "        return delta\n",
    "    \n",
    "    def compute_gradients(self, delta):\n",
    "        dW = np.matmul(self._input.T, delta)\n",
    "        db = np.matmul(np.ones(self._input.shape[0]), delta)\n",
    "        return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    # 各層共通のパラメータを設定\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.layer_1 = Layer(input_dim, hidden_dim, sigmoid, desigmoid)\n",
    "        self.layer_2 = Layer(hidden_dim, output_dim, sigmoid, desigmoid)\n",
    "        self.layers = [self.layer_1, self.layer_2]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.layer_1.forward(x)\n",
    "        o = self.layer_2.forward(h)\n",
    "        return o \n",
    "    \n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    \n",
    "def desigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimuramiyuki/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss:  inf\n",
      "epoch: 11, loss:  inf\n",
      "epoch: 21, loss:  inf\n",
      "epoch: 31, loss:  inf\n",
      "epoch: 41, loss:  inf\n",
      "epoch: 51, loss:  inf\n",
      "epoch: 61, loss:  inf\n",
      "epoch: 71, loss:  inf\n",
      "epoch: 81, loss:  inf\n",
      "epoch: 91, loss:  inf\n",
      "epoch: 101, loss:  inf\n",
      "epoch: 111, loss:  inf\n",
      "epoch: 121, loss:  inf\n",
      "epoch: 131, loss:  inf\n",
      "epoch: 141, loss:  inf\n",
      "epoch: 151, loss:  inf\n",
      "epoch: 161, loss:  inf\n",
      "epoch: 171, loss:  inf\n",
      "epoch: 181, loss:  inf\n",
      "epoch: 191, loss:  inf\n",
      "epoch: 201, loss:  inf\n",
      "epoch: 211, loss:  inf\n",
      "epoch: 221, loss:  inf\n",
      "epoch: 231, loss:  inf\n",
      "epoch: 241, loss:  inf\n",
      "epoch: 251, loss:  inf\n",
      "epoch: 261, loss:  inf\n",
      "epoch: 271, loss:  inf\n",
      "epoch: 281, loss:  inf\n",
      "epoch: 291, loss:  inf\n",
      "epoch: 301, loss:  inf\n",
      "epoch: 311, loss:  inf\n",
      "epoch: 321, loss:  inf\n",
      "epoch: 331, loss:  inf\n",
      "epoch: 341, loss:  inf\n",
      "epoch: 351, loss:  inf\n",
      "epoch: 361, loss:  inf\n",
      "epoch: 371, loss:  inf\n",
      "epoch: 381, loss:  inf\n",
      "epoch: 391, loss:  inf\n",
      "epoch: 401, loss:  inf\n",
      "epoch: 411, loss:  inf\n",
      "epoch: 421, loss:  inf\n",
      "epoch: 431, loss:  inf\n",
      "epoch: 441, loss:  inf\n",
      "epoch: 451, loss:  inf\n",
      "epoch: 461, loss:  inf\n",
      "epoch: 471, loss:  inf\n",
      "epoch: 481, loss:  inf\n",
      "epoch: 491, loss:  inf\n",
      "epoch: 501, loss:  inf\n",
      "epoch: 511, loss:  inf\n",
      "epoch: 521, loss:  inf\n",
      "epoch: 531, loss:  inf\n",
      "epoch: 541, loss:  inf\n",
      "epoch: 551, loss:  inf\n",
      "epoch: 561, loss:  inf\n",
      "epoch: 571, loss:  inf\n",
      "epoch: 581, loss:  inf\n",
      "epoch: 591, loss:  inf\n",
      "epoch: 601, loss:  inf\n",
      "epoch: 611, loss:  inf\n",
      "epoch: 621, loss:  inf\n",
      "epoch: 631, loss:  inf\n",
      "epoch: 641, loss:  inf\n",
      "epoch: 651, loss:  inf\n",
      "epoch: 661, loss:  inf\n",
      "epoch: 671, loss:  inf\n",
      "epoch: 681, loss:  inf\n",
      "epoch: 691, loss:  inf\n",
      "epoch: 701, loss:  inf\n",
      "epoch: 711, loss:  inf\n",
      "epoch: 721, loss:  inf\n",
      "epoch: 731, loss:  inf\n",
      "epoch: 741, loss:  inf\n",
      "epoch: 751, loss:  inf\n",
      "epoch: 761, loss:  inf\n",
      "epoch: 771, loss:  inf\n",
      "epoch: 781, loss:  inf\n",
      "epoch: 791, loss:  inf\n",
      "epoch: 801, loss:  inf\n",
      "epoch: 811, loss:  inf\n",
      "epoch: 821, loss:  inf\n",
      "epoch: 831, loss:  inf\n",
      "epoch: 841, loss:  inf\n",
      "epoch: 851, loss:  inf\n",
      "epoch: 861, loss:  inf\n",
      "epoch: 871, loss:  inf\n",
      "epoch: 881, loss:  inf\n",
      "epoch: 891, loss:  inf\n",
      "epoch: 901, loss:  inf\n",
      "epoch: 911, loss:  inf\n",
      "epoch: 921, loss:  inf\n",
      "epoch: 931, loss:  inf\n",
      "epoch: 941, loss:  inf\n",
      "epoch: 951, loss:  inf\n",
      "epoch: 961, loss:  inf\n",
      "epoch: 971, loss:  inf\n",
      "epoch: 981, loss:  inf\n",
      "epoch: 991, loss:  inf\n",
      "epoch: 1001, loss:  inf\n",
      "epoch: 1011, loss:  inf\n",
      "epoch: 1021, loss:  inf\n",
      "epoch: 1031, loss:  inf\n",
      "epoch: 1041, loss:  inf\n",
      "epoch: 1051, loss:  inf\n",
      "epoch: 1061, loss:  inf\n",
      "epoch: 1071, loss:  inf\n",
      "epoch: 1081, loss:  inf\n",
      "epoch: 1091, loss:  inf\n",
      "epoch: 1101, loss:  inf\n",
      "epoch: 1111, loss:  inf\n",
      "epoch: 1121, loss:  inf\n",
      "epoch: 1131, loss:  inf\n",
      "epoch: 1141, loss:  inf\n",
      "epoch: 1151, loss:  inf\n",
      "epoch: 1161, loss:  inf\n",
      "epoch: 1171, loss:  inf\n",
      "epoch: 1181, loss:  inf\n",
      "epoch: 1191, loss:  inf\n",
      "epoch: 1201, loss:  inf\n",
      "epoch: 1211, loss:  inf\n",
      "epoch: 1221, loss:  inf\n",
      "epoch: 1231, loss:  inf\n",
      "epoch: 1241, loss:  inf\n",
      "epoch: 1251, loss:  inf\n",
      "epoch: 1261, loss:  inf\n",
      "epoch: 1271, loss:  inf\n",
      "epoch: 1281, loss:  inf\n",
      "epoch: 1291, loss:  inf\n",
      "epoch: 1301, loss:  inf\n",
      "epoch: 1311, loss:  inf\n",
      "epoch: 1321, loss:  inf\n",
      "epoch: 1331, loss:  inf\n",
      "epoch: 1341, loss:  inf\n",
      "epoch: 1351, loss:  inf\n",
      "epoch: 1361, loss:  inf\n",
      "epoch: 1371, loss:  inf\n",
      "epoch: 1381, loss:  inf\n",
      "epoch: 1391, loss:  inf\n",
      "epoch: 1401, loss:  inf\n",
      "epoch: 1411, loss:  inf\n",
      "epoch: 1421, loss:  inf\n",
      "epoch: 1431, loss:  inf\n",
      "epoch: 1441, loss:  inf\n",
      "epoch: 1451, loss:  inf\n",
      "epoch: 1461, loss:  inf\n",
      "epoch: 1471, loss:  inf\n",
      "epoch: 1481, loss:  inf\n",
      "epoch: 1491, loss:  inf\n",
      "epoch: 1501, loss:  inf\n",
      "epoch: 1511, loss:  inf\n",
      "epoch: 1521, loss:  inf\n",
      "epoch: 1531, loss:  inf\n",
      "epoch: 1541, loss:  inf\n",
      "epoch: 1551, loss:  inf\n",
      "epoch: 1561, loss:  inf\n",
      "epoch: 1571, loss:  inf\n",
      "epoch: 1581, loss:  inf\n",
      "epoch: 1591, loss:  inf\n",
      "epoch: 1601, loss:  inf\n",
      "epoch: 1611, loss:  inf\n",
      "epoch: 1621, loss:  inf\n",
      "epoch: 1631, loss:  inf\n",
      "epoch: 1641, loss:  inf\n",
      "epoch: 1651, loss:  inf\n",
      "epoch: 1661, loss:  inf\n",
      "epoch: 1671, loss:  inf\n",
      "epoch: 1681, loss:  inf\n",
      "epoch: 1691, loss:  inf\n",
      "epoch: 1701, loss:  inf\n",
      "epoch: 1711, loss:  inf\n",
      "epoch: 1721, loss:  inf\n",
      "epoch: 1731, loss:  inf\n",
      "epoch: 1741, loss:  inf\n",
      "epoch: 1751, loss:  inf\n",
      "epoch: 1761, loss:  inf\n",
      "epoch: 1771, loss:  inf\n",
      "epoch: 1781, loss:  inf\n",
      "epoch: 1791, loss:  inf\n",
      "epoch: 1801, loss:  inf\n",
      "epoch: 1811, loss:  inf\n",
      "epoch: 1821, loss:  inf\n",
      "epoch: 1831, loss:  inf\n",
      "epoch: 1841, loss:  inf\n",
      "epoch: 1851, loss:  inf\n",
      "epoch: 1861, loss:  inf\n",
      "epoch: 1871, loss:  inf\n",
      "epoch: 1881, loss:  inf\n",
      "epoch: 1891, loss:  inf\n",
      "epoch: 1901, loss:  inf\n",
      "epoch: 1911, loss:  inf\n",
      "epoch: 1921, loss:  inf\n",
      "epoch: 1931, loss:  inf\n",
      "epoch: 1941, loss:  inf\n",
      "epoch: 1951, loss:  inf\n",
      "epoch: 1961, loss:  inf\n",
      "epoch: 1971, loss:  inf\n",
      "epoch: 1981, loss:  inf\n",
      "epoch: 1991, loss:  inf\n",
      "epoch: 2001, loss:  inf\n",
      "epoch: 2011, loss:  inf\n",
      "epoch: 2021, loss:  inf\n",
      "epoch: 2031, loss:  inf\n",
      "epoch: 2041, loss:  inf\n",
      "epoch: 2051, loss:  inf\n",
      "epoch: 2061, loss:  inf\n",
      "epoch: 2071, loss:  inf\n",
      "epoch: 2081, loss:  inf\n",
      "epoch: 2091, loss:  inf\n",
      "epoch: 2101, loss:  inf\n",
      "epoch: 2111, loss:  inf\n",
      "epoch: 2121, loss:  inf\n",
      "epoch: 2131, loss:  inf\n",
      "epoch: 2141, loss:  inf\n",
      "epoch: 2151, loss:  inf\n",
      "epoch: 2161, loss:  inf\n",
      "epoch: 2171, loss:  inf\n",
      "epoch: 2181, loss:  inf\n",
      "epoch: 2191, loss:  inf\n",
      "epoch: 2201, loss:  inf\n",
      "epoch: 2211, loss:  inf\n",
      "epoch: 2221, loss:  inf\n",
      "epoch: 2231, loss:  inf\n",
      "epoch: 2241, loss:  inf\n",
      "epoch: 2251, loss:  inf\n",
      "epoch: 2261, loss:  inf\n",
      "epoch: 2271, loss:  inf\n",
      "epoch: 2281, loss:  inf\n",
      "epoch: 2291, loss:  inf\n",
      "epoch: 2301, loss:  inf\n",
      "epoch: 2311, loss:  inf\n",
      "epoch: 2321, loss:  inf\n",
      "epoch: 2331, loss:  inf\n",
      "epoch: 2341, loss:  inf\n",
      "epoch: 2351, loss:  inf\n",
      "epoch: 2361, loss:  inf\n",
      "epoch: 2371, loss:  inf\n",
      "epoch: 2381, loss:  inf\n",
      "epoch: 2391, loss:  inf\n",
      "epoch: 2401, loss:  inf\n",
      "epoch: 2411, loss:  inf\n",
      "epoch: 2421, loss:  inf\n",
      "epoch: 2431, loss:  inf\n",
      "epoch: 2441, loss:  inf\n",
      "epoch: 2451, loss:  inf\n",
      "epoch: 2461, loss:  inf\n",
      "epoch: 2471, loss:  inf\n",
      "epoch: 2481, loss:  inf\n",
      "epoch: 2491, loss:  inf\n",
      "epoch: 2501, loss:  inf\n",
      "epoch: 2511, loss:  inf\n",
      "epoch: 2521, loss:  inf\n",
      "epoch: 2531, loss:  inf\n",
      "epoch: 2541, loss:  inf\n",
      "epoch: 2551, loss:  inf\n",
      "epoch: 2561, loss:  inf\n",
      "epoch: 2571, loss:  inf\n",
      "epoch: 2581, loss:  inf\n",
      "epoch: 2591, loss:  inf\n",
      "epoch: 2601, loss:  inf\n",
      "epoch: 2611, loss:  inf\n",
      "epoch: 2621, loss:  inf\n",
      "epoch: 2631, loss:  inf\n",
      "epoch: 2641, loss:  inf\n",
      "epoch: 2651, loss:  inf\n",
      "epoch: 2661, loss:  inf\n",
      "epoch: 2671, loss:  inf\n",
      "epoch: 2681, loss:  inf\n",
      "epoch: 2691, loss:  inf\n",
      "epoch: 2701, loss:  inf\n",
      "epoch: 2711, loss:  inf\n",
      "epoch: 2721, loss:  inf\n",
      "epoch: 2731, loss:  inf\n",
      "epoch: 2741, loss:  inf\n",
      "epoch: 2751, loss:  inf\n",
      "epoch: 2761, loss:  inf\n",
      "epoch: 2771, loss:  inf\n",
      "epoch: 2781, loss:  inf\n",
      "epoch: 2791, loss:  inf\n",
      "epoch: 2801, loss:  inf\n",
      "epoch: 2811, loss:  inf\n",
      "epoch: 2821, loss:  inf\n",
      "epoch: 2831, loss:  inf\n",
      "epoch: 2841, loss:  inf\n",
      "epoch: 2851, loss:  inf\n",
      "epoch: 2861, loss:  inf\n",
      "epoch: 2871, loss:  inf\n",
      "epoch: 2881, loss:  inf\n",
      "epoch: 2891, loss:  inf\n",
      "epoch: 2901, loss:  inf\n",
      "epoch: 2911, loss:  inf\n",
      "epoch: 2921, loss:  inf\n",
      "epoch: 2931, loss:  inf\n",
      "epoch: 2941, loss:  inf\n",
      "epoch: 2951, loss:  inf\n",
      "epoch: 2961, loss:  inf\n",
      "epoch: 2971, loss:  inf\n",
      "epoch: 2981, loss:  inf\n",
      "epoch: 2991, loss:  inf\n",
      "epoch: 3001, loss:  inf\n",
      "epoch: 3011, loss:  inf\n",
      "epoch: 3021, loss:  inf\n",
      "epoch: 3031, loss:  inf\n",
      "epoch: 3041, loss:  inf\n",
      "epoch: 3051, loss:  inf\n",
      "epoch: 3061, loss:  inf\n",
      "epoch: 3071, loss:  inf\n",
      "epoch: 3081, loss:  inf\n",
      "epoch: 3091, loss:  inf\n",
      "epoch: 3101, loss:  inf\n",
      "epoch: 3111, loss:  inf\n",
      "epoch: 3121, loss:  inf\n",
      "epoch: 3131, loss:  inf\n",
      "epoch: 3141, loss:  inf\n",
      "epoch: 3151, loss:  inf\n",
      "epoch: 3161, loss:  inf\n",
      "epoch: 3171, loss:  inf\n",
      "epoch: 3181, loss:  inf\n",
      "epoch: 3191, loss:  inf\n",
      "epoch: 3201, loss:  inf\n",
      "epoch: 3211, loss:  inf\n",
      "epoch: 3221, loss:  inf\n",
      "epoch: 3231, loss:  inf\n",
      "epoch: 3241, loss:  inf\n",
      "epoch: 3251, loss:  inf\n",
      "epoch: 3261, loss:  inf\n",
      "epoch: 3271, loss:  inf\n",
      "epoch: 3281, loss:  inf\n",
      "epoch: 3291, loss:  inf\n",
      "epoch: 3301, loss:  inf\n",
      "epoch: 3311, loss:  inf\n",
      "epoch: 3321, loss:  inf\n",
      "epoch: 3331, loss:  inf\n",
      "epoch: 3341, loss:  inf\n",
      "epoch: 3351, loss:  inf\n",
      "epoch: 3361, loss:  inf\n",
      "epoch: 3371, loss:  inf\n",
      "epoch: 3381, loss:  inf\n",
      "epoch: 3391, loss:  inf\n",
      "epoch: 3401, loss:  inf\n",
      "epoch: 3411, loss:  inf\n",
      "epoch: 3421, loss:  inf\n",
      "epoch: 3431, loss:  inf\n",
      "epoch: 3441, loss:  inf\n",
      "epoch: 3451, loss:  inf\n",
      "epoch: 3461, loss:  inf\n",
      "epoch: 3471, loss:  inf\n",
      "epoch: 3481, loss:  inf\n",
      "epoch: 3491, loss:  inf\n",
      "epoch: 3501, loss:  inf\n",
      "epoch: 3511, loss:  inf\n",
      "epoch: 3521, loss:  inf\n",
      "epoch: 3531, loss:  inf\n",
      "epoch: 3541, loss:  inf\n",
      "epoch: 3551, loss:  inf\n",
      "epoch: 3561, loss:  inf\n",
      "epoch: 3571, loss:  inf\n",
      "epoch: 3581, loss:  inf\n",
      "epoch: 3591, loss:  inf\n",
      "epoch: 3601, loss:  inf\n",
      "epoch: 3611, loss:  inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3621, loss:  inf\n",
      "epoch: 3631, loss:  inf\n",
      "epoch: 3641, loss:  inf\n",
      "epoch: 3651, loss:  inf\n",
      "epoch: 3661, loss:  inf\n",
      "epoch: 3671, loss:  inf\n",
      "epoch: 3681, loss:  inf\n",
      "epoch: 3691, loss:  inf\n",
      "epoch: 3701, loss:  inf\n",
      "epoch: 3711, loss:  inf\n",
      "epoch: 3721, loss:  inf\n",
      "epoch: 3731, loss:  inf\n",
      "epoch: 3741, loss:  inf\n",
      "epoch: 3751, loss:  inf\n",
      "epoch: 3761, loss:  inf\n",
      "epoch: 3771, loss:  inf\n",
      "epoch: 3781, loss:  inf\n",
      "epoch: 3791, loss:  inf\n",
      "epoch: 3801, loss:  inf\n",
      "epoch: 3811, loss:  inf\n",
      "epoch: 3821, loss:  inf\n",
      "epoch: 3831, loss:  inf\n",
      "epoch: 3841, loss:  inf\n",
      "epoch: 3851, loss:  inf\n",
      "epoch: 3861, loss:  inf\n",
      "epoch: 3871, loss:  inf\n",
      "epoch: 3881, loss:  inf\n",
      "epoch: 3891, loss:  inf\n",
      "epoch: 3901, loss:  inf\n",
      "epoch: 3911, loss:  inf\n",
      "epoch: 3921, loss:  inf\n",
      "epoch: 3931, loss:  inf\n",
      "epoch: 3941, loss:  inf\n",
      "epoch: 3951, loss:  inf\n",
      "epoch: 3961, loss:  inf\n",
      "epoch: 3971, loss:  inf\n",
      "epoch: 3981, loss:  inf\n",
      "epoch: 3991, loss:  inf\n",
      "epoch: 4001, loss:  inf\n",
      "epoch: 4011, loss:  inf\n",
      "epoch: 4021, loss:  inf\n",
      "epoch: 4031, loss:  inf\n",
      "epoch: 4041, loss:  inf\n",
      "epoch: 4051, loss:  inf\n",
      "epoch: 4061, loss:  inf\n",
      "epoch: 4071, loss:  inf\n",
      "epoch: 4081, loss:  inf\n",
      "epoch: 4091, loss:  inf\n",
      "epoch: 4101, loss:  inf\n",
      "epoch: 4111, loss:  inf\n",
      "epoch: 4121, loss:  inf\n",
      "epoch: 4131, loss:  inf\n",
      "epoch: 4141, loss:  inf\n",
      "epoch: 4151, loss:  inf\n",
      "epoch: 4161, loss:  inf\n",
      "epoch: 4171, loss:  inf\n",
      "epoch: 4181, loss:  inf\n",
      "epoch: 4191, loss:  inf\n",
      "epoch: 4201, loss:  inf\n",
      "epoch: 4211, loss:  inf\n",
      "epoch: 4221, loss:  inf\n",
      "epoch: 4231, loss:  inf\n",
      "epoch: 4241, loss:  inf\n",
      "epoch: 4251, loss:  inf\n",
      "epoch: 4261, loss:  inf\n",
      "epoch: 4271, loss:  inf\n",
      "epoch: 4281, loss:  inf\n",
      "epoch: 4291, loss:  inf\n",
      "epoch: 4301, loss:  inf\n",
      "epoch: 4311, loss:  inf\n",
      "epoch: 4321, loss:  inf\n",
      "epoch: 4331, loss:  inf\n",
      "epoch: 4341, loss:  inf\n",
      "epoch: 4351, loss:  inf\n",
      "epoch: 4361, loss:  inf\n",
      "epoch: 4371, loss:  inf\n",
      "epoch: 4381, loss:  inf\n",
      "epoch: 4391, loss:  inf\n",
      "epoch: 4401, loss:  inf\n",
      "epoch: 4411, loss:  inf\n",
      "epoch: 4421, loss:  inf\n",
      "epoch: 4431, loss:  inf\n",
      "epoch: 4441, loss:  inf\n",
      "epoch: 4451, loss:  inf\n",
      "epoch: 4461, loss:  inf\n",
      "epoch: 4471, loss:  inf\n",
      "epoch: 4481, loss:  inf\n",
      "epoch: 4491, loss:  inf\n",
      "epoch: 4501, loss:  inf\n",
      "epoch: 4511, loss:  inf\n",
      "epoch: 4521, loss:  inf\n",
      "epoch: 4531, loss:  inf\n",
      "epoch: 4541, loss:  inf\n",
      "epoch: 4551, loss:  inf\n",
      "epoch: 4561, loss:  inf\n",
      "epoch: 4571, loss:  inf\n",
      "epoch: 4581, loss:  inf\n",
      "epoch: 4591, loss:  inf\n",
      "epoch: 4601, loss:  inf\n",
      "epoch: 4611, loss:  inf\n",
      "epoch: 4621, loss:  inf\n",
      "epoch: 4631, loss:  inf\n",
      "epoch: 4641, loss:  inf\n",
      "epoch: 4651, loss:  inf\n",
      "epoch: 4661, loss:  inf\n",
      "epoch: 4671, loss:  inf\n",
      "epoch: 4681, loss:  inf\n",
      "epoch: 4691, loss:  inf\n",
      "epoch: 4701, loss:  inf\n",
      "epoch: 4711, loss:  inf\n",
      "epoch: 4721, loss:  inf\n",
      "epoch: 4731, loss:  inf\n",
      "epoch: 4741, loss:  inf\n",
      "epoch: 4751, loss:  inf\n",
      "epoch: 4761, loss:  inf\n",
      "epoch: 4771, loss:  inf\n",
      "epoch: 4781, loss:  inf\n",
      "epoch: 4791, loss:  inf\n",
      "epoch: 4801, loss:  inf\n",
      "epoch: 4811, loss:  inf\n",
      "epoch: 4821, loss:  inf\n",
      "epoch: 4831, loss:  inf\n",
      "epoch: 4841, loss:  inf\n",
      "epoch: 4851, loss:  inf\n",
      "epoch: 4861, loss:  inf\n",
      "epoch: 4871, loss:  inf\n",
      "epoch: 4881, loss:  inf\n",
      "epoch: 4891, loss:  inf\n",
      "epoch: 4901, loss:  inf\n",
      "epoch: 4911, loss:  inf\n",
      "epoch: 4921, loss:  inf\n",
      "epoch: 4931, loss:  inf\n",
      "epoch: 4941, loss:  inf\n",
      "epoch: 4951, loss:  inf\n",
      "epoch: 4961, loss:  inf\n",
      "epoch: 4971, loss:  inf\n",
      "epoch: 4981, loss:  inf\n",
      "epoch: 4991, loss:  inf\n",
      "epoch: 5000, loss:  inf\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.データ準備\n",
    "'''\n",
    "# xor\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "t = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "'''\n",
    "2. モデル準備\n",
    "'''\n",
    "model = MLP(2, 2, 1)\n",
    "\n",
    "'''\n",
    "3. 学習\n",
    "'''\n",
    "def compute_loss(y, t):\n",
    "    return (-t * np.log(y) - (1 - t) * np.log(1 - y)).sum()\n",
    "\n",
    "def train_step(x, t):\n",
    "    y = model(x)\n",
    "    # 逆伝播なので逆から回す\n",
    "    for i, layer in enumerate(model.layers[::-1]):\n",
    "        if i == 0:\n",
    "            delta = y - t\n",
    "        else:\n",
    "            delta = layer.backward(delta, W)\n",
    "        \n",
    "        dW, db = layer.compute_gradients(delta)\n",
    "        layer.W = layer.W - 0.1*dW\n",
    "        layer.b = layer.b - 0.1*db\n",
    "        \n",
    "        W = layer.W\n",
    "    \n",
    "    loss = compute_loss(t, y)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_step(x, t)\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print('epoch: {}, loss: {: .3f}'.format(\n",
    "            epoch+1, \n",
    "            train_loss\n",
    "        ))\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[0.00486133]\n",
      "[0 1]\n",
      "[0.99366436]\n",
      "[1 0]\n",
      "[0.99369749]\n",
      "[1 1]\n",
      "[0.00557292]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "4. モデルの評価\n",
    "'''\n",
    "for input in x:\n",
    "    print(input)\n",
    "    print(model.forward(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
